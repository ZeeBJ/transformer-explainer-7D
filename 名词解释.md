# Transformer关键术语解释

## 概述

本文档详细说明 Transformer Explainer 项目中从 **Embedding（嵌入）** 到 **Probabilities（概率）** 的完整流程，并对每个关键英文术语提供中文解释。

---

## 一、Embedding（嵌入）

### 1.1 Token（词元）
**定义**：Token 是文本被分割成的最小处理单元。可以是完整的词（word）或词的子部分（subword）。

**示例**：
- 完整词：`"Data"`、`"visualization"` 各自对应一个 token
- 子词：`"empowers"` 可能被分割成两个 token

**参数说明**：
- **Vocabulary Size（词汇表大小）**：GPT-2 的词汇表包含 **50,257** 个唯一的 token
- **Token ID**：每个 token 在词汇表中都有唯一的数字标识符

### 1.2 Tokenization（词元化）
**定义**：将输入文本分解为 token 的过程。

**作用**：
- 将自然语言文本转换为模型可以处理的数字序列
- 每个 token 被分配一个唯一的 ID

### 1.3 Token Embedding（词元嵌入）
**定义**：将每个 token 的 ID 转换为数值向量的过程。

**关键参数**：
- **Embedding Dimension（嵌入维度）**：GPT-2 (small) 使用 **768 维**向量表示每个 token
- **Embedding Matrix（嵌入矩阵）**：形状为 `(50,257, 768)` 的矩阵
  - 第一维：词汇表大小（50,257）
  - 第二维：嵌入维度（768）
  - **总参数数量**：约 3,900 万个参数

**数学表示**：
```
Token Embedding Matrix: W_te ∈ R^(vocab_size × d_model)
其中 vocab_size = 50,257, d_model = 768
```

### 1.4 Positional Encoding（位置编码）
**定义**：为每个 token 在序列中的位置信息进行编码。

**作用**：
- Transformer 模型本身不包含位置信息，需要通过位置编码来告知模型每个 token 的位置
- GPT-2 从头开始训练自己的位置编码矩阵

**关键参数**：
- **Position Embedding Matrix（位置嵌入矩阵）**：形状为 `(block_size, 768)`
  - `block_size`：最大序列长度（GPT-2 为 1024）

### 1.5 Final Embedding（最终嵌入）
**定义**：将 Token Embedding 和 Positional Encoding 相加得到的最终表示。

**计算公式**：
```
Final Embedding = Token Embedding + Positional Encoding
```

**输出形状**：`(batch_size, sequence_length, 768)`

---

## 二、Transformer Block（Transformer 块）

### 2.1 Block（块）
**定义**：Transformer 模型的基本处理单元，包含多头自注意力和 MLP 层。

**关键参数**：
- **Number of Blocks（块数量）**：GPT-2 (small) 包含 **12** 个 Transformer 块
- **Block Size（块大小）**：最大序列长度，GPT-2 为 **1024**

### 2.2 Layer Normalization（层归一化）
**定义**：对每层的输入进行归一化处理，稳定训练过程。

**作用**：
- 减少内部协变量偏移（Internal Covariate Shift）
- 加速模型收敛
- 降低对初始权重的敏感性

**应用位置**：
- 自注意力机制之前
- MLP 层之前
- 最终输出之前

### 2.3 Residual Connection（残差连接）
**定义**：将层的输入直接添加到输出，形成"跳跃连接"。

**作用**：
- 缓解梯度消失问题
- 使深层网络更容易训练
- 允许梯度直接流过网络

**数学表示**：
```
output = Layer(input) + input
```

---

## 三、Multi-Head Self-Attention（多头自注意力）

### 3.1 Query, Key, Value（查询、键、值）
**定义**：每个 token 的嵌入向量被转换为三个向量：Q（Query）、K（Key）、V（Value）。

**类比理解**：
- **Query (Q)**：类似搜索引擎中的搜索词，表示"我想查找什么信息"
- **Key (K)**：类似搜索结果中的标题，表示"我可以提供什么信息"
- **Value (V)**：类似搜索结果的实际内容，表示"具体的信息内容"

**关键参数**：
- **QKV 矩阵维度**：每个都是 `(sequence_length, 768)`
- **权重矩阵**：通过训练学习得到，用于从嵌入生成 Q、K、V

**计算公式**：
```
Q = Embedding × W_q
K = Embedding × W_k
V = Embedding × W_v
```

### 3.2 Multi-Head（多头）
**定义**：将 Q、K、V 向量分割成多个独立的"头"，每个头关注不同的模式。

**关键参数**：
- **Number of Heads（头数量）**：GPT-2 (small) 使用 **12** 个头
- **Head Dimension（头维度）**：每个头的维度 = 768 / 12 = **64**

**Attention Head 编号说明**：
- 在多头注意力机制中，每个头都有唯一的编号，从 **0** 开始到 **11**（共 12 个头）
- **Attention Head 4 Out** 中的 **"4"** 表示**第 5 个注意力头**的输出（编号从 0 开始，所以 4 是第 5 个）
- 每个头独立计算注意力，捕获不同的语言特征：
  - **Head 0-3**：可能更关注局部语法关系
  - **Head 4-7**：可能更关注语义关系
  - **Head 8-11**：可能更关注长距离依赖关系
- 不同头关注不同的模式，这种设计使模型能够并行学习多种语言特征

**作用**：
- 每个头可以捕获不同的语言特征（语法、语义、长距离依赖等）
- 并行处理，提高模型表示能力
- 多头设计增强了模型的表示能力和泛化性能

### 3.3 Attention Score（注意力分数）
**定义**：Query 和 Key 矩阵的点积，表示每个 token 对其他 token 的关注程度。

**计算公式**：
```
Attention Score = Q × K^T / √d_k
```
其中 `d_k` 是 Key 的维度（64）

**输出形状**：`(sequence_length, sequence_length)` 的方阵

**含义**：
- 矩阵中的每个值表示一个 token 对另一个 token 的注意力权重
- 值越大，表示关注程度越高

### 3.4 Masking（掩码）
**定义**：在注意力矩阵的上三角部分应用掩码，将未来 token 的注意力分数设置为负无穷。

**作用**：
- 防止模型在生成时"偷看"未来的 token
- 确保模型只能使用当前位置之前的信息进行预测
- 这是自回归（autoregressive）模型的关键特性

**数学表示**：
```
Masked Attention[i, j] = {
    Attention[i, j]  if j <= i
    -∞              if j > i
}
```

### 3.5 Softmax（Softmax 函数）
**定义**：将注意力分数转换为概率分布的函数。

**计算公式**：
```
Softmax(x_i) = exp(x_i) / Σ exp(x_j)
```

**作用**：
- 将注意力分数转换为 0 到 1 之间的概率值
- 每行的概率值总和为 1
- 表示每个 token 对其他 token 的相对重要性

**输出形状**：`(sequence_length, sequence_length)`，每行和为 1

### 3.6 Attention Output（注意力输出）
**定义**：将 Softmax 后的注意力权重与 Value 矩阵相乘得到的输出。

**计算公式**：
```
Attention Output = Softmax(QK^T / √d_k) × V
```

**输出形状**：`(sequence_length, 768)`

### 3.7 Concatenation（拼接）
**定义**：将多个注意力头的输出拼接在一起。

**关键参数**：
- **12 个头的输出**：每个头输出 `(sequence_length, 64)`
- **拼接后**：`(sequence_length, 768)`

**作用**：
- 整合所有头捕获的不同特征
- 通过线性投影层进一步处理

---

## 四、MLP（多层感知器）

### 4.1 MLP（Multi-Layer Perceptron，多层感知器）
**定义**：一个前馈神经网络，独立地处理每个 token 的表示。

**作用**：
- 精炼每个 token 的表示
- 与注意力机制配合：注意力负责 token 间的信息路由，MLP 负责 token 内部的表示精炼

**关键参数**：
- **输入维度**：768
- **中间层维度**：通常为 3072（4 倍扩展）
- **输出维度**：768

**结构**：
```
Input (768) → Linear → GELU → Linear → Output (768)
              ↑                    ↑
            (3072)              (3072)
```

### 4.2 GELU（Gaussian Error Linear Unit）
**定义**：MLP 中使用的激活函数。

**特点**：
- 比 ReLU 更平滑
- 在 GPT-2 中用于非线性变换

---

## 五、Output Probabilities（输出概率）

### 5.1 Linear Layer（线性层）
**定义**：将处理后的嵌入投影到词汇表大小的空间。

**关键参数**：
- **输入维度**：768
- **输出维度**：50,257（词汇表大小）

**作用**：
- 将每个 token 的 768 维表示转换为 50,257 维的 logits

### 5.2 Logit（逻辑值）
**定义**：线性层输出的原始分数，表示每个 token 成为下一个词的可能性。

**特点**：
- 未归一化的分数
- 可以是任意实数（正数或负数）
- 值越大，表示该 token 越可能成为下一个词

**输出形状**：`(sequence_length, 50,257)`

### 5.3 Softmax（Softmax 函数）
**定义**：将 logits 转换为概率分布的函数。

**计算公式**：
```
Probability(token_i) = exp(logit_i) / Σ exp(logit_j)
```

**作用**：
- 将 logits 转换为 0 到 1 之间的概率值
- 所有 token 的概率总和为 1
- 表示每个 token 成为下一个词的概率

**输出形状**：`(sequence_length, 50,257)`，每行和为 1

### 5.4 Temperature（温度参数）
**定义**：控制输出概率分布形状的超参数。

**计算公式**：
```
Adjusted Logit = Logit / Temperature
```

**参数值的影响**：
- **Temperature = 1**：不改变 softmax 输出，保持原始概率分布
- **Temperature < 1**（如 0.5）：
  - 使概率分布更尖锐（sharp）
  - 模型更自信、更确定性
  - 输出更可预测
- **Temperature > 1**（如 1.5）：
  - 使概率分布更平滑（soft）
  - 增加随机性
  - 输出更具"创造性"

**作用**：平衡模型的确定性和多样性

### 5.5 Top-k Sampling（Top-k 采样）
**定义**：只从概率最高的 k 个 token 中进行采样。

**参数**：
- **k**：保留的 top token 数量（如 k=40）

**作用**：
- 过滤掉不太可能的 token
- 在保持多样性的同时提高输出质量
- 减少低概率 token 的干扰

### 5.6 Top-p Sampling（Top-p 采样，核采样）
**定义**：从累积概率超过阈值 p 的最小 token 集合中采样。

**参数**：
- **p**：累积概率阈值（如 p=0.9）

**工作原理**：
1. 按概率从高到低排序所有 token
2. 累加概率直到总和 ≥ p
3. 只从这个集合中采样

**作用**：
- 动态调整候选 token 数量
- 确保只考虑最可能的 token
- 同时保持多样性

### 5.7 Sampling（采样）
**定义**：根据概率分布随机选择下一个 token 的过程。

**方法**：
- **Greedy Sampling（贪婪采样）**：总是选择概率最高的 token（确定性）
- **Random Sampling（随机采样）**：根据概率分布随机选择（随机性）

**作用**：
- 从概率分布中生成下一个 token
- 完成文本生成过程

---

## 六、关键参数总结

### 模型架构参数（GPT-2 Small）

| 参数名称 | 英文 | 数值 | 说明 |
|---------|------|------|------|
| 词汇表大小 | Vocabulary Size | 50,257 | 唯一 token 的数量 |
| 嵌入维度 | Embedding Dimension | 768 | 每个 token 的向量维度 |
| Transformer 块数 | Number of Blocks | 12 | 堆叠的 Transformer 层数 |
| 注意力头数 | Number of Heads | 12 | 多头注意力的头数 |
| 头维度 | Head Dimension | 64 | 每个头的维度（768/12） |
| 最大序列长度 | Block Size | 1024 | 可处理的最大 token 数 |
| MLP 中间层维度 | MLP Hidden Dimension | 3072 | MLP 的扩展维度 |
| 模型参数总数 | Total Parameters | ~124M | 约 1.24 亿个参数 |

### 生成参数

| 参数名称 | 英文 | 典型值 | 说明 |
|---------|------|--------|------|
| 温度 | Temperature | 0.7-1.5 | 控制输出随机性 |
| Top-k | Top-k | 40 | 保留的 top token 数 |
| Top-p | Top-p | 0.9 | 累积概率阈值 |

---

## 七、完整流程总结

```
输入文本
    ↓
Tokenization（词元化）
    ↓
Token Embedding（词元嵌入） + Positional Encoding（位置编码）
    ↓
Final Embedding（最终嵌入）
    ↓
[Transformer Block × 12]
    ├─ Multi-Head Self-Attention（多头自注意力）
    │   ├─ Q, K, V（查询、键、值）
    │   ├─ Attention Score（注意力分数）
    │   ├─ Masking（掩码）
    │   ├─ Softmax（Softmax）
    │   └─ Attention Output（注意力输出）
    ├─ MLP（多层感知器）
    └─ Residual Connection（残差连接）
    ↓
Linear Layer（线性层）
    ↓
Logits（逻辑值）
    ↓
Softmax（Softmax）
    ↓
Probabilities（概率）
    ↓
Temperature（温度调整）
    ↓
Top-k / Top-p Sampling（采样）
    ↓
输出下一个 Token
```

---

## 八、术语对照表

| 英文术语 | 中文翻译 | 缩写/别名 |
|---------|---------|----------|
| Embedding | 嵌入 | - |
| Token | 词元 | - |
| Tokenization | 词元化 | - |
| Positional Encoding | 位置编码 | PE |
| Transformer Block | Transformer 块 | - |
| Multi-Head Self-Attention | 多头自注意力 | MHSA |
| Attention Head | 注意力头 | Head |
| Attention Head N Out | 第 N 个注意力头输出 | Head N Out (N=0-11) |
| Query | 查询 | Q |
| Key | 键 | K |
| Value | 值 | V |
| Attention Score | 注意力分数 | - |
| Masking | 掩码 | - |
| Softmax | Softmax | - |
| MLP | 多层感知器 | Multi-Layer Perceptron |
| Layer Normalization | 层归一化 | LayerNorm |
| Residual Connection | 残差连接 | Skip Connection |
| Linear Layer | 线性层 | - |
| Logit | 逻辑值 | - |
| Probability | 概率 | Prob |
| Temperature | 温度 | T |
| Top-k Sampling | Top-k 采样 | - |
| Top-p Sampling | Top-p 采样 | Nucleus Sampling |
| Sampling | 采样 | - |

---

## 九、参考资料

- **论文**：[Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)
- **GPT-2 论文**：[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- **项目地址**：https://github.com/poloclub/transformer-explainer
- **在线演示**：http://poloclub.github.io/transformer-explainer


